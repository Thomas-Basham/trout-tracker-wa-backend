# ğŸŸ Troutlytics Backend

[![Python application](https://github.com/troutlytics/troutlytics-backend/actions/workflows/python-app.yml/badge.svg)](https://github.com/troutlytics/troutlytics-backend/actions/workflows/python-app.yml)

## Description

**Troutlytics** is a data-driven Python application that scrapes and stores trout stocking data for Washington State lakes. It runs on a scheduled AWS Fargate task and stores results in an Aurora PostgreSQL database for use in dashboards, maps, and analysis tools.

---

## ğŸ“¦ Project Structure

```bash
.
â”œâ”€â”€ api/                          # ğŸ¯ Main application API
â”‚   â”œâ”€â”€ __init__.py              # API package initializer
â”‚   â”œâ”€â”€ index.py                 # API entrypoint (routes/controllers)
â”‚   â”œâ”€â”€ requirements.txt         # API dependencies
â”‚   â”œâ”€â”€ dockerfiles/
â”‚   â”‚   â”œâ”€â”€ dev/Dockerfile       # Dev Dockerfile
â”‚   â”‚   â””â”€â”€ prod/                # Production Dockerfile (Lambda-ready)
â”‚   â”‚       â”œâ”€â”€ Dockerfile
â”‚   â”‚       â””â”€â”€ lambda_entry_script.sh
â”‚   â””â”€â”€ README.md                # API-specific usage docs

â”œâ”€â”€ web_scraper/                 # ğŸ•¸ï¸ Web scraping service
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper.py              # Main script for collecting trout/creel data
â”‚   â”œâ”€â”€ Dockerfile              # Docker setup for scraper
â”‚   â”œâ”€â”€ Makefile                # Shortcuts for common dev tasks
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ tests/                  # ğŸ”¬ Pytest-based tests
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ test_scraper.py

â”œâ”€â”€ data/                        # ğŸ—ƒï¸ Database models and storage
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py             # SQLAlchemy engine and session config
â”‚   â”œâ”€â”€ models.py               # ORM models for tables
â”‚   â”œâ”€â”€ backup_data.sql         # SQL dump for backup or restore
â”‚   â”œâ”€â”€ backup_data.txt         # Raw text backup
â”‚   â””â”€â”€ sqlite.db               # Local development database

â”œâ”€â”€ aws_config/                 # â˜ï¸ AWS deployment and secrets setup
â”‚   â”œâ”€â”€ configure-aws-credentials-latest.yml  # GitHub Actions for AWS login
â”‚   â””â”€â”€ fargate-rds-secrets.yaml              # Fargate setup with RDS and Secrets Manager served there)
â”œâ”€â”€ README.md              # You are here ğŸ“˜
```

â¸»

ğŸš€ Deployment Overview

AWS Infrastructure:

- Fargate runs the scraper every 24 hours via EventBridge.
- Secrets Manager securely stores DB credentials.
- Aurora PostgreSQL stores structured stocking data.
- CloudWatch Logs tracks runtime output for visibility.
- API hosted with API Gateway and Lambda

GitHub â†’ ECR Workflow:

- Automatically builds and pushes Docker image on main branch updates.
- Uses secure OIDC GitHub Actions role to push to ECR.

â¸»

ğŸ“‹ Prerequisites

- AWS CLI configured with appropriate permissions
- Docker installed (for local dev builds)
- Python 3.11+

â¸»

ğŸ§ª Run Locally

## ğŸš€ Docker Compose Commands Cheat Sheet

Everything is ran from the root repo folder

| Action                       | Command                            | Notes                                         |
| :--------------------------- | :--------------------------------- | :-------------------------------------------- |
| **Build all services**       | `docker compose build`             | Build all images                              |
| **Start all services**       | `docker compose up`                | Start API(s), Scraper                         |
| **Start all + rebuild**      | `docker compose up --build`        | Force rebuild before starting                 |
| **Start dev API only**       | `docker compose up api-dev`        | Starts API Dev service                        |
| **Start prod API only**      | `docker compose up api-prod`       | Starts API Prod service                       |
| **Start scraper only**       | `docker compose up web-scraper`    | Starts Scraper                                |
| **Stop all services**        | `docker compose down`              | Stops and removes all containers and networks |
| **Rebuild dev API only**     | `docker compose build api-dev`     | Rebuild only the dev API image                |
| **Rebuild prod API only**    | `docker compose build api-prod`    | Rebuild only the prod API image               |
| **Rebuild scraper only**     | `docker compose build web-scraper` | Rebuild only the scraper image                |
| **View running containers**  | `docker compose ps`                | Show status of all services                   |
| **View logs (all services)** | `docker compose logs`              | View logs for all services                    |
| **Follow logs live**         | `docker compose logs -f`           | Stream logs in real time                      |
| **Stop dev API**             | `docker compose stop api-dev`      | Stop only the dev API container               |
| **Stop prod API**            | `docker compose stop api-prod`     | Stop only the prod API container              |
| **Stop scraper**             | `docker compose stop web-scraper`  | Stop only the scraper container               |
| **Restart all containers**   | `docker compose restart`           | Restart all running services                  |

---

## ğŸ› ï¸ Cloud Setup

Deploy the CloudFormation Stack:

```bash
aws cloudformation deploy \
 --template-file aws_config/fargate-rds-secrets.yaml \
 --stack-name troutlytics-scraper \
 --capabilities CAPABILITY_NAMED_IAM \
 --parameter-overrides \
 ECRImageUri=<your-ecr-image-uri> \
 VpcId=<your-vpc-id> \
 SubnetIds=subnet-xxxx,subnet-yyyy \
 SecurityGroupId=sg-xxxxxx \
 SecretArn=arn:aws:secretsmanager:us-west-2:xxx:secret:troutlytics-db
```

â¸»

ğŸ” GitHub â†’ ECR Deploy (CI/CD)

To enable GitHub Actions auto-deploy:

1. Deploy the github_oidc_ecr_access.yaml CloudFormation template.
2. Add the output IAM Role ARN to your GitHub Actions secrets or workflows.
3. Push to main â€” your image builds and publishes to ECR automatically.

â¸»

ğŸ“ˆ Roadmap Ideas

- Add support for weather/streamflow overlays
- Enable historical trend analysis by lake
- Integrate public stocking alerts
- Expand scraper coverage to other regions or species

â¸»

ğŸ§  Credits

Created by @thomas-basham â€” U.S. Army veteran, full-stack developer, and passionate angler ğŸ£

â¸»

License

MIT
